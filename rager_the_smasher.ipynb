{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d5a05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from  dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8205a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key is set\n"
     ]
    }
   ],
   "source": [
    "if os.environ['OPEN_API_SECRET_KEY']:\n",
    "    print(\"OpenAI API key is set\")\n",
    "else:\n",
    "    print(\"OpenAI API key is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f314eb0",
   "metadata": {},
   "source": [
    "## We will use local llm model ollama instead of openai paid, so any of you can test locally without spending bucks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142725d",
   "metadata": {},
   "source": [
    "#### Let's implement a RAG system using local LLMs via Ollama and interacting with a SQL database using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f817c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "What is the total revenue of Headphones, aggregate on product name?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mHere's the answer:\n",
      "\n",
      "Question: What is the total revenue of Headphones, aggregate on product_name?\n",
      "SQLQuery:\n",
      "SELECT \"product_name\", SUM(\"price\") AS \"Total Revenue\"\n",
      "FROM \"Sales\"\n",
      "WHERE \"product_name\" = 'Headphones'\n",
      "GROUP BY \"product_name\";\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[('Headphones', 200.0)]\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Result:  [('Headphones', 200.0)]\n"
     ]
    }
   ],
   "source": [
    "## incase you are using openai api\n",
    "# from langchain_openai import ChatOpenAI \n",
    "# llm = ChatOpenAI(model_name=\"gpt-<version>\", temperature=0)\n",
    "\n",
    "# from langchain_ollama import ChatOllama\n",
    "## incase you are using ollama api\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\", temperature=0)\n",
    "database = SQLDatabase.from_uri(\"sqlite:///demo.db\")\n",
    "chain = SQLDatabaseChain(llm=llm, database=database, verbose=True, return_direct=True)\n",
    "\n",
    "query = (\"What is the total revenue of Headphones, aggregate on product name?\")\n",
    "\n",
    "result = chain.run(query)\n",
    "print(\"Result: \", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ab53f",
   "metadata": {},
   "source": [
    "# Let's implement a RAG system using local LLMs via Ollama and interacting with a SQL database using LangChain.\n",
    "#### Step 1: We will pass our text to the LLM and get the response. (Prepare Document for your text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a1cbfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=' The team lead is Owais Ajaz in QoreLogix, now you guys will see the magic of local llm. before context the llm didnt \n",
      "but we have build the context now be happy' metadata={'source': 'wikipedia', 'version': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Your text data\n",
    "my_text = \"\"\" The team lead is Owais Ajaz in QoreLogix, now you guys will see the magic of local llm. before context the llm didnt \n",
    "but we have build the context now be happy\"\"\"\n",
    "\n",
    "doc = Document(page_content=my_text, metadata={\"source\": \"wikipedia\", \"version\": 1})\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20bd6d9",
   "metadata": {},
   "source": [
    "#### Step 2: Splitting the document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ad83a5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'wikipedia', 'version': 1}, page_content='The team lead is Owais Ajaz in QoreLogix, now you guys will see the magic of local llm. before context the llm didnt \\nbut we have build the context now be happy')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunks = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=25,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ").split_documents([doc])\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644e3c3",
   "metadata": {},
   "source": [
    "#### Step 3: Now its tom time to build a vector database, we will convert the chunks to embeddings and store them in a vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "721818e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "## incase you are using openai embeddings\n",
    "# embeddings_model = OpenAIEmbeddings(model=\"text-embedding-5-small\")\n",
    "# embeddings_model.embed_query(\"Hello world\")\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings_model = OllamaEmbeddings(model=\"llama3\")\n",
    "# embeddings_model.embed_query(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa92e3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'version': 1, 'source': 'wikipedia'}, page_content='The team lead is M Owais Ajaz in qorelogix, now you guys will see the magic of local llm. before context the llm didnt \\nbut we have build the context now be happy'),\n",
       " Document(metadata={'source': 'wikipedia', 'version': 1}, page_content='For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings_model\n",
    "    # collection_name=\"my_collection\",\n",
    "    # persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "context = vectorstore.similarity_search(\"who is team lead in QoreLogix?\", k=2)\n",
    "\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f48cf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but you didn't provide any context. Could you please provide more information about QoreLogix and what you mean by \"Team Lead\"? I'll do my best to help you with your question.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(f\"who is team lead in QoreLogix? use the context provided: {context}\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
