{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d5a05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from  dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8205a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key is set\n"
     ]
    }
   ],
   "source": [
    "if os.environ['OPEN_API_SECRET_KEY']:\n",
    "    print(\"OpenAI API key is set\")\n",
    "else:\n",
    "    print(\"OpenAI API key is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f314eb0",
   "metadata": {},
   "source": [
    "## We will use local llm model ollama instead of openai paid, so any of you can test locally without spending bucks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142725d",
   "metadata": {},
   "source": [
    "#### Let's implement a RAG system using local LLMs via Ollama and interacting with a SQL database using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f817c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "What is the total revenue of Headphones, aggregate on product name?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mHere's the answer:\n",
      "\n",
      "Question: What is the total revenue of Headphones, aggregate on product_name?\n",
      "SQLQuery:\n",
      "SELECT \"product_name\", SUM(\"price\") AS \"Total Revenue\"\n",
      "FROM \"Sales\"\n",
      "WHERE \"product_name\" = 'Headphones'\n",
      "GROUP BY \"product_name\";\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[('Headphones', 200.0)]\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Result:  [('Headphones', 200.0)]\n"
     ]
    }
   ],
   "source": [
    "## incase you are using openai api\n",
    "# from langchain_openai import ChatOpenAI \n",
    "# llm = ChatOpenAI(model_name=\"gpt-<version>\", temperature=0)\n",
    "\n",
    "# from langchain_ollama import ChatOllama\n",
    "## incase you are using ollama api\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\", temperature=0)\n",
    "database = SQLDatabase.from_uri(\"sqlite:///demo.db\")\n",
    "chain = SQLDatabaseChain(llm=llm, database=database, verbose=True, return_direct=True)\n",
    "\n",
    "query = (\"What is the total revenue of Headphones, aggregate on product name?\")\n",
    "\n",
    "result = chain.run(query)\n",
    "print(\"Result: \", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ab53f",
   "metadata": {},
   "source": [
    "# Let's implement a RAG system using local LLMs via Ollama and interacting with a SQL database using LangChain.\n",
    "#### Step 1: We will pass our text to the LLM and get the response. (Prepare Document for your text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a1cbfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='YRetrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to \n",
      "retrieve and incorporate new information from external data sources.[1] With RAG, LLMs do not respond to user \n",
      "queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing t\n",
      "raining data.[2] This allows LLMs to use domain-specific and/or updated information that is not available in the training data.[2] \n",
      "For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.' metadata={'source': 'wikipedia', 'version': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Your text data\n",
    "my_text = \"\"\"YRetrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to \n",
    "retrieve and incorporate new information from external data sources.[1] With RAG, LLMs do not respond to user \n",
    "queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing t\n",
    "raining data.[2] This allows LLMs to use domain-specific and/or updated information that is not available in the training data.[2] \n",
    "For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\"\"\"\n",
    "\n",
    "doc = Document(page_content=my_text, metadata={\"source\": \"wikipedia\", \"version\": 1})\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20bd6d9",
   "metadata": {},
   "source": [
    "#### Step 2: Splitting the document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad83a5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'wikipedia', 'version': 1}, page_content=\"YRetrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to \\nretrieve and incorporate new information from external data sources.[1] With RAG, LLMs do not respond to user \\nqueries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing t\\nraining data.[2] This allows LLMs to use domain-specific and/or updated information that is not available in the training data.[2]\"),\n",
       " Document(metadata={'source': 'wikipedia', 'version': 1}, page_content='For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunks = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=25,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ").split_documents([doc])\n",
    "\n",
    "chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
